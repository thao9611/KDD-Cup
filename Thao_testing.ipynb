{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import data_io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from feature_set import *\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "stopword = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#from Thao_features import *\n",
    "dataset = {}\n",
    "dataset['paper'] = pd.read_csv('dataRev2/Paper.csv')[:2000]\n",
    "trainset = pd.read_csv('dataRev2/Train.csv')\n",
    "train_confirmed = trainset[['AuthorId', 'ConfirmedPaperIds']].rename(columns = {'ConfirmedPaperIds':'PaperIds'})\n",
    "targetset = train_confirmed\n",
    "dataset['paper_author'] = pd.read_csv('dataRev2/PaperAuthor1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for keyword of Papers\n",
    "def filter_keyword(text):\n",
    "    for i in string.punctuation: text = text.replace(i,' ')\n",
    "    words = word_tokenize(text) #split words\n",
    "    words = [w.lower() for w in words if w.isalpha()] #get rid of punctuation\n",
    "    words = [w for w in words if w not in  [\"keywords\"] ]\n",
    "    stemmed = [porter.stem(w) for w in words]\n",
    "    return stemmed\n",
    "\n",
    "# for title of papers\n",
    "def tokenize(text):\n",
    "    words = word_tokenize(text.decode(\"utf8\")) #split words\n",
    "    words = [w.lower() for w in words if w.isalpha()] #get rid of punctuation\n",
    "    words =[w for w in words if  not w in stopword]\n",
    "    stemmed = [porter.stem(w) for w in words]\n",
    "    return stemmed\n",
    "\n",
    "#return the keywords of each paper\n",
    "def paper_keywords(data):\n",
    "    paper = data['paper']\n",
    "    paperid = list(paper[\"Id\"])\n",
    "    paper_keyword = defaultdict(list)\n",
    "\n",
    "    paper= paper.set_index(\"Id\")\n",
    "    paper['Keyword']= paper['Keyword'].fillna(\"\")\n",
    "    paper['Title']= paper['Title'].fillna(\"\")\n",
    "    title = list(paper[\"Title\"])\n",
    "    paper['Token'] = paper.Title.map(tokenize)\n",
    "    paper['Keyword_pro'] = paper['Keyword'].map(filter_keyword)\n",
    "\n",
    "    #concatenate keyword and token\n",
    "    paper['Key_token'] = paper[['Keyword_pro','Token']].apply((lambda x: ' '.join(list(set([i for z in x for i in z])))), axis =1)\n",
    "    token = list(paper['Key_token'])\n",
    "    count = CountVectorizer(min_df = 5) #only take words with df > 5\n",
    "    tfidf = TfidfTransformer()\n",
    "    count_token =count.fit_transform(token).toarray() #2000*527\n",
    "    #tfid_token = tfidf.fit_transform(count_token)\n",
    "    vocab = list(count.vocabulary_.keys())\n",
    "    #list of common words in each title of each document\n",
    "    paper['Common word'] = paper['Key_token'].map(lambda x: [i for i in x.split() if i in vocab])\n",
    "    for i in paperid:\n",
    "        paper_keyword[i] = paper.loc[i,'Common word']\n",
    "    return paper_keyword\n",
    "\n",
    "#how similar two documents are based on keywords\n",
    "def paper_common_word(data, id1, id2):\n",
    "    paper_keyword = paper_keywords(data)\n",
    "    sim = 0\n",
    "    word1 = paper_keyword[id1]\n",
    "    word2 = paper_keyword[id2]\n",
    "    for i in word1:\n",
    "        if i in word2:\n",
    "            sim += 1\n",
    "    return sim\n",
    "\n",
    "\n",
    "def target_paper_and_papers_of_target_author_by_keywords(dataset,author_paper_pairs):\n",
    "    paper_sim = defaultdict(int)\n",
    "    trainset = dataset['paper_author']\n",
    "   \n",
    "    for i in author_paper_pairs:\n",
    "        trained_paper= list(trainset.loc[trainset[\"AuthorId\"]== i[0], \"PaperId\"])\n",
    "        paper_sim[i] = sum(paper_common_word(dataset,i[1],j) for j in trained_paper)\n",
    "    return paper_sim\n",
    "\n",
    "def parse_paper_ids(paper_ids_string):\n",
    "    return paper_ids_string.strip().split()\n",
    "\n",
    "def parse_targetset(targetset):\n",
    "    pair_list = []\n",
    "    author_id_list = targetset['AuthorId']\n",
    "\n",
    "    for i in range(len(author_id_list)):\n",
    "        author_id = author_id_list[i]\n",
    "        papers = targetset[targetset.AuthorId == author_id]['PaperIds'].unique()[0]\n",
    "        papers = parse_paper_ids(papers)\n",
    "        for j in range(len(papers)):\n",
    "            paper_id = int(papers[j])\n",
    "            pair_list.append( (author_id, paper_id) )\n",
    "    return list(set(pair_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_paper_pairs = parse_targetset(targetset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thao_f3 = target_paper_and_confirmed_papers_of_target_author_by_keywords(dataset,author_paper_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}