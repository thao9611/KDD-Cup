{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from extract_feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataset['paper'] = pd.read_csv('dataRev2/Paper.csv')\n",
    "dataset['author'] = pd.read_csv('dataRev2/Author.csv')\n",
    "dataset['conference'] = pd.read_csv('dataRev2/Conference.csv')\n",
    "dataset['journal'] = pd.read_csv('dataRev2/Journal.csv')\n",
    "dataset['paper_author'] = pd.read_csv('dataRev2/PaperAuthor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_feature_list(author_paper_pairs, ap_to_feature_list):\n",
    "    result_list = []\n",
    "\n",
    "    temp_dict = {} # { (author, paper) => [f1, f2 ...] }\n",
    "    for ap_pair in author_paper_pairs:\n",
    "        temp_dict[ap_pair] = []\n",
    "\n",
    "    for i in range(len(ap_to_feature_list)):\n",
    "        feature_dict = ap_to_feature_list[i]\n",
    "        for ap_pair in author_paper_pairs:\n",
    "            feature = feature_dict[ap_pair]\n",
    "            temp_dict[ap_pair].append(feature)\n",
    "\n",
    "    for key in temp_dict.keys():\n",
    "        result_list.append(key + tuple( temp_dict[key] ))\n",
    "\n",
    "    return result_list\n",
    "\n",
    "def kamil_new_f1(dataset, author_paper_pairs):\n",
    "    feature_dict = {}\n",
    "\n",
    "    #aid_to_count = dataset['paper_author']['AuthorId'].value_counts()\n",
    "    print(author_paper_pairs[0])\n",
    "    for ap_pair in author_paper_pairs:\n",
    "        feature_dict[ap_pair] = int(ap_pair[0]) + int(ap_pair[1])\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "def get_features(dataset, targetset):\n",
    "    author_paper_pairs = parse_targetset(targetset)\n",
    "\n",
    "    # Keep the format of f# (dictionary): { (a1, p1): feature_value1, (a2, p2): feature_value2 ... }\n",
    "    # Add your features here and add them to feature_list!\n",
    "    harry_f1 = get_author_publishes_how_many_paper_in_PaperAuthor(dataset, author_paper_pairs)\n",
    "    harry_f2 = get_paper_has_how_many_author_in_PaperAuthor(dataset, author_paper_pairs)\n",
    "    \n",
    "    #harry_list = [harry_f1, harry_f2]\n",
    "    harry_list = []\n",
    "    \n",
    "    \n",
    "    kamil_f1 = kamil_new_f1(dataset, author_paper_pairs)\n",
    "    kamil_list = [kamil_f1]\n",
    "    \n",
    "    thao_list = []\n",
    "    feature_list = harry_list + kamil_list + thao_list\n",
    "\n",
    "    result_list = generate_feature_list(author_paper_pairs, feature_list)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting features for deleted papers from the database\n",
      "(1539933, 1359549)\n"
     ]
    }
   ],
   "source": [
    "trainset = pd.read_csv('dataRev2/Train.csv')\n",
    "train_confirmed = trainset[['AuthorId', 'ConfirmedPaperIds']].rename(columns = {'ConfirmedPaperIds':'PaperIds'})\n",
    "print(\"Getting features for deleted papers from the database\")\n",
    "features_conf = get_features(dataset, train_confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1539933, 1359549, 7, 190),\n",
       " (1455231, 467172, 6, 267),\n",
       " (1215636, 1791266, 4, 446),\n",
       " (205278, 1737961, 4, 34),\n",
       " (433821, 1901940, 3, 262)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_conf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paper_author.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pa_only = paper_author[['PaperId','AuthorId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_relation(author_id,paper_id):\n",
    "    relation_list = pd.DataFrame()\n",
    "    #find papers related to the author\n",
    "    related_papers = pa_only[pa_only.AuthorId == author_id]['PaperId'].unique()\n",
    "    print(\"# related papers:\",related_papers.shape)\n",
    "    \n",
    "    #find authors related to the paper\n",
    "    related_authors = paper_author[paper_author['PaperId'] == paper_id]['AuthorId'].unique()\n",
    "    print(\"# related authors:\",related_authors.shape)\n",
    "    \n",
    "    #remove target paper and target author from lists\n",
    "    related_papers = np.delete(related_papers,np.argwhere(related_papers == paper_id))\n",
    "    related_authors = np.delete(related_authors, np.argwhere(related_authors == author_id))\n",
    "    \n",
    "    print(\"# related papers:\",related_papers.shape)\n",
    "    print(\"# related authors:\",related_authors.shape)\n",
    "    cnt = 0\n",
    "    for author in related_authors:\n",
    "        for paper in related_papers:\n",
    "            single_relation = paper_author[(paper_author['PaperId'] == paper) & (paper_author['AuthorId'] == author)]\n",
    "            if(single_relation.size > 0):\n",
    "                #display(single_relation)\n",
    "                relation_list = relation_list.append(single_relation)\n",
    "                break\n",
    "        \"\"\"\n",
    "        cnt += 1\n",
    "        if cnt> 55:\n",
    "            break\n",
    "        \"\"\"\n",
    "    print(relation_list.shape)\n",
    "    print(relation_list['AuthorId'].shape)\n",
    "    \n",
    "    return relation_list\n",
    "\n",
    "#target author, target paper\n",
    "d_list = [{\"author\" : \"826\", \"confirmed\" : \"25733 47276 77012\"},\n",
    "          {\"author\" : \"1118\",\"confirmed\" : \"49963 93433 341015 415282\"}\n",
    "         ]\n",
    "\n",
    "for d in d_list:\n",
    "    author,confirmed = d[\"author\"],d[\"confirmed\"]\n",
    "    for x in confirmed.split():\n",
    "        tmp = find_relation(int(author),int(x))\n",
    "        display(tmp)\n",
    "        l.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit find_relation(826,25733)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
